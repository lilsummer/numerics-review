{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam Study Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a final exam study guide that covers the material from chapters 7 through 13 of the textbook _Scientific Computing_ by Michael Heath <cite data-cite=\"heath2018scientific\">(Heath, 2018)</cite>.  These notes were originally compiled by Marco Morais while taking [CS 450 Numerical Analysis](https://cs.illinois.edu/sites/default/files/CS450_NumericalAnalysis.pdf) at UIUC and come without any guarantee of accuracy or endorsement by the textbook author.\n",
    "\n",
    "```\n",
    "@book{heath2018scientific,\n",
    "  title={Scientific computing: an introductory survey},\n",
    "  author={Heath, Michael T},\n",
    "  volume={80},\n",
    "  year={2018},\n",
    "  publisher={SIAM}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation: Problem Statement\n",
    "Given input data such as:\n",
    "$$\n",
    "(t_1, y_1), (t_2, y_2), \\cdots, (t_m, y_m) \\quad t_1 < t_2 < \\cdots < t_m\n",
    "$$\n",
    "\n",
    "determine a function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ such that:\n",
    "$$\n",
    "f(t_i) = y_i \\quad i = 1, \\cdots, m\n",
    "$$\n",
    "\n",
    "$f$ is referred to as **interpolating function**\n",
    "\n",
    "Interpolating functions are **exact** fit at sample points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Functions\n",
    "Family of functions for interpolating data points spanned by a set of basis functions $\\phi_1(t), \\cdots, \\phi_n(t)$.\n",
    "\n",
    "Interpolating function $f$ is linear combination of basis functions:\n",
    "$$\n",
    "f(t_i) = \\sum_{j=1}^{n} x_j \\phi_j (t_i) = y_i\n",
    "$$\n",
    "where\n",
    "* $x$ is an n-vector of parameters\n",
    "* $\\phi_j (t_i)$ is the value of *jth* basis function at $t_i$ forms an $m \\times n$ matrix $A$\n",
    "* $y$ is the product of the linear system $Ax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existence, Uniqueness, and Conditioning\n",
    "Existence depends on the number of data points $m$ and basis functions $n$.\n",
    "* If $m > n$, then interpolant usually doesn't exist.\n",
    "* If $m < n$, then interpolant is not unique.\n",
    "* If $m = n$, then basis matrix $A$ is nonsingular and data can be fit exactly.\n",
    "\n",
    "Sensitivity of $x$ depends on $\\text{cond}(A)$ which depends on basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Interpolation\n",
    "Polynomial of degree $n-1$ passsing through $n$ distinct data points is unique.\n",
    "* Although the polynomial is unique, the representation is not.  Some choices described below.\n",
    "\n",
    "---\n",
    "#### 1- Monomial Basis\n",
    "Basis functions given by sequential powers:\n",
    "$$\n",
    "\\phi_j (t) = t^{j-1} \\quad j=1, \\cdots, n\n",
    "$$\n",
    "\n",
    "Polynomial evaluated at $t_i$ with this basis:\n",
    "$$\n",
    "p_{n-1} (t_i) = x_1 + x_2 t_i + x_3 t_i^2 + \\cdots + x_n t_i^{n-1}\n",
    "$$\n",
    "\n",
    "The $n \\times n$ matrix is known as **Vandermonde matrix**:\n",
    "$$\n",
    "Ax = \\begin{bmatrix}\n",
    "1 & t_1 & t_1^2 & \\cdots & t_1^{n-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & t_n & t_n^2 & \\cdots & t_n^{n-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For monomial basis, matrix $\\text{cond}(A)$ increases with degree of polynomial.\n",
    "* Ill-conditioning does not prevent us from fitting points.\n",
    "* Residuals at fitted data points is zero.\n",
    "* Scaling can help reduce the growth in condition number.\n",
    "\n",
    "Cost of finding interpolant: $O(n^3)$ (solve linear system)\n",
    "\n",
    "Cost of evaluating interpolant: $2n$ (Horner's rule)\n",
    "\n",
    "---\n",
    "#### 2- Lagrange Basis\n",
    "The Vandermonde matrix for the Lagrange basis is the identity matrix $I$.\n",
    "$$\n",
    "l_j(t) = \\prod_{k=1,k \\neq j}^n (t - t_k) / \\prod_{k=1,k \\neq j}^n (t_j - t_k) \\quad j = 1,\\cdots,n\n",
    "$$\n",
    "\n",
    "Polynomial evaluated at $t_i$ with this basis:\n",
    "$$\n",
    "p_{n-1}(t_i) = y_1 l_1(t_i) + y_2 l_2(t_i) + \\cdots + y_n l_n(t_i)\n",
    "$$\n",
    "\n",
    "Basis matrix is the identity matrix $I$, thus very well conditioned. \n",
    "\n",
    "Cost of finding interpolant: $O(n^2)$\n",
    "\n",
    "Cost of evaluating interpolant: $5n$\n",
    "\n",
    "---\n",
    "#### 3- Newton Basis\n",
    "Newton basis functions given by:\n",
    "$$\n",
    "\\pi_j(t) = \\prod_{k=1}^{j-1} (t - t_k) \\quad j = 1,\\cdots,n\n",
    "$$\n",
    "\n",
    "Polynomial evaluated at $t_i$ with this basis:\n",
    "$$\n",
    "p_{n-1}(t_i) = x_1 + x_2(t - t_1) + x_3(t - t_1)(t - t_2) + \\cdots + x_n(t - t_1)(t - t_2)\\cdots(t - t_{n-1})\n",
    "$$\n",
    "* Avoid repeated computations using Horner's rule.\n",
    "\n",
    "The basis matrix $A$ formed from the linear combination of these basis functions is lower triangular and can be solved using forward substitution (cost = $O(n^2)$).\n",
    "\n",
    "Newton basis functions can be constructed incrementally.\n",
    "\n",
    "Cost of finding interpolant: $O(n^2)$ (lower triangular)\n",
    "\n",
    "Cost of evaluating interpolant: $2n$ (Horner's rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horner's Rule\n",
    "Assume a polynomial is represented as a vector of coefficients ordered from highest degree to the constant.\n",
    "\n",
    "Horner's rule is used to efficiently evaluate a polynomial of the form\n",
    "$$\n",
    "p(x) = p_0 x^{n-1} + p_1 x^{n-2} + \\cdots + p_{n-2} x + p_{n-1}\n",
    "$$\n",
    "\n",
    "using the following algebraically equivalent formula\n",
    "$$\n",
    "p(x) = p_{n-1} + x(p_{n-2} + x(\\cdots + x(p_1 + p_0 x)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Polynomial Interpolation\n",
    "Fit large number of samples with low-degree polynomials to avoid excessive oscillations in the interpolant.\n",
    "* Break the interval $[t_1, t_n]$ into $k$ subintervals.\n",
    "* Each point at which interpolant changes referred to as **knot** or **breakpoint**.\n",
    "\n",
    "#### Cubic Interpolation\n",
    "* Assume $n$ knots with $n-1$ piecewise subintervals.\n",
    "* For each knot, there are 4 parameters, $x$, in the cubic eg $p_{3}(t) = x_0 + x_1 t + x_2 t^2 + x_3 t^3$.\n",
    "* As a result, there are $4(n-1)$ parameters to be determined.\n",
    "\n",
    "Hermite and cubic spline variants discussed below.\n",
    "\n",
    "---\n",
    "#### 1- Hermite Cubic Interpolation\n",
    "Hermite cubic interpolant is piecewise cubic polynomial interpolant.\n",
    "* Uses only first derivative.\n",
    "* $n$ free parameters to be chosen.\n",
    "* To preserve monotonic data, then Hermite cubic is appropriate.\n",
    "\n",
    "---\n",
    "#### 2- Cubic Spline Interpolation\n",
    "Cubic spline is piecewise cubic polynomial interpolant.\n",
    "* Uses first and second derivative.\n",
    "  * **Natural spline** forces second derivative to be zero at endpoints.\n",
    "* 2 free parameters to be chosen.\n",
    "* For maximum smoothness, then a cubic spline is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 Topics Not Covered\n",
    "* Orthogonal Polynomials\n",
    "* (Truncated) Taylor Series\n",
    "* B-splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 Numerical Integration and Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration: Problem Statement\n",
    "For $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ the definite integral over interval $[a, b]$:\n",
    "$$\n",
    "I(f) = \\int_a^b f(x) dx\n",
    "$$\n",
    "\n",
    "is defined by the limit of Riemann sums shown below\n",
    "$$\n",
    "R_n = \\sum_{i=1}^n (x_{i+1} - x_i) f(\\xi_i) \\quad n \\rightarrow \\infty\n",
    "$$\n",
    "\n",
    "Integration is a smoothing operation and always well-conditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Cotes Quadrature\n",
    "Quadrature rules based on equally spaced nodes and fixed weights in interval $[a, b]$.\n",
    "\n",
    "---\n",
    "#### 1 - Midpoint Rule\n",
    "n=1 (odd), open\n",
    "$$\n",
    "M(f) = (b-a)\\, f \\left( \\frac{a+b}{2} \\right)\n",
    "$$\n",
    "\n",
    "Error:  $E_M(f) = F(f) - M(f)$ \n",
    "\n",
    "Midpoint about twice as accurate as trapezoid rule, despite smaller $n$.\n",
    "  * Degree of odd rules is $n$ vs even rules is $n-1$.\n",
    "  * Due to cancellation of positive and negative errors.\n",
    "\n",
    "---\n",
    "#### 2- Trapezoid Rule\n",
    "n=2 (even), closed\n",
    "$$\n",
    "T(f) = \\frac{b-a}{2}\\, \\left( f(a) + f(b) \\right)\n",
    "$$\n",
    "\n",
    "Error:  $E_T(f) = F(f) - T(f)$ \n",
    "\n",
    "Relating error between $T(f)$ and $M(f)$\n",
    "$$\n",
    "E(f) = \\frac{T(f) - M(f)}{3}\n",
    "$$\n",
    "\n",
    "---\n",
    "#### 3- Simpson's Rule\n",
    "n=3 (odd), closed\n",
    "$$\n",
    "S(f) = \\frac{b-a}{6}\\, \\left( f(a) + 4\\,f(\\frac{a+b}{2}) + f(b) \\right)\n",
    "$$\n",
    "\n",
    "Error:  $E_S(f) = F(f) - S(f)$ \n",
    "\n",
    "Computing $S(f)$ from $M(f)$ and $T(f)$\n",
    "$$\n",
    "S(f) = \\frac{2}{3}M(f) + \\frac{1}{3}T(f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Quadrature\n",
    "Nodes and weights are chosen to maximize degree.\n",
    "  * Unlike Newton-Cotes in which nodes are fixed and equally spaced.\n",
    "  * Highest possible accuracy (compared to Newton-Cotes) for the number of nodes used.\n",
    "  * Weights are always positive.\n",
    "  * Not progressive eg when the number of nodes is increased, say from $n$ to $m$, $m$ new evaluations of the integrand are required.\n",
    "  \n",
    "Degree: An n-point Gaussian quadrature rule is of degree $2n - 1$.\n",
    "\n",
    "Gaussian quadrature rules of different orders have at most 1 point in common.\n",
    "  * If $m$ and $n$ are odd and of different order, then midpoint for any two Gaussian quadrature rules $G_m$ and $G_n$ will be the same.  No other points are common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite Quadrature\n",
    "Equivalent to piecewise interpolation for quadrature. \n",
    "  * Subdivide original interval $[a, b]$  into $k$ subintervals of length $h=(b-a)/k$.\n",
    "  * Apply Newton-Cotes quadrature rule in each subinterval.\n",
    "  * Can be made **progressive** eg reuse computation across intervals.\n",
    "\n",
    "Composite midpoint rule\n",
    "$$\n",
    "M_k(f) = h \\sum_{j=1}^k f \\left( \\frac{x_{j-1} + x_j}{2} \\right)\n",
    "$$\n",
    "\n",
    "Composite trapezoid rule\n",
    "$$\n",
    "T_k(f) = h \\left( \\frac{1}{2} f(a) + f(x_1) + \\cdots + f(x_{k-1}) + \\frac{1}{2} f(b) \\right)\n",
    "$$\n",
    "\n",
    "The nodes along the interval are given by: $x_j = a + jh, \\quad j=0, \\cdots, k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Integration\n",
    "For integrands over arbitrary multiple intervals, use Monte Carlo method.\n",
    "* Sample $n$ points distributed randomly overly interval.\n",
    "* Evaluate the function at the $n$ points to obtain a mean value.\n",
    "* Multiply the mean value by the area or volume of the domain to obtain an estimate of the integral.\n",
    "\n",
    "Advantages\n",
    "* Works for functions with discontinuities.\n",
    "* Convergence rate is independent of the number of dimensions, hence effective for high dimensions.\n",
    "\n",
    "Disadvantages\n",
    "* Error in estimate goes to zero as $1/\\sqrt{n}$ or roughly 1 decimal digit for 100-fold increase in the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation: Finite Differences\n",
    "Given a smooth function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ and step size $h$ approximate its first and second derivatives.\n",
    "\n",
    "---\n",
    "#### 1- First Order Accurate\n",
    "\n",
    "Forward Difference Approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "Backward Difference Approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x) - f(x-h)}{h}\n",
    "$$\n",
    "\n",
    "---\n",
    "#### 2- Second Order Accurate\n",
    "\n",
    "Centered Difference Approximation of First Derivative\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
    "$$\n",
    "\n",
    "Centered Difference Approximation of Second Derivative\n",
    "$$\n",
    "f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 Topics Not Covered\n",
    "* Clenshaw-Curtis Quadrature\n",
    "* Adaptive Quadrature\n",
    "* Gauss-Kronrod rules\n",
    "* Integral Equations\n",
    "* Automatic Differentiation aka autodiff\n",
    "* Romberg Integration via Richardson Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 Initial Value Problems for ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Equations: Problem Statement\n",
    "Suppose that the state of a system is described by a vector-valued function $y : \\mathbb{R} \\rightarrow \\mathbb{R}^n$ where the scalar domain of the function is typically something like time, $t$.\n",
    "\n",
    "A **differential equation** describes the relationship between some function $y(t)$ and one or more of its derivatives with respect to $t$.\n",
    "$$\n",
    "y'(t) = \\frac{dy}{dt}\n",
    "$$\n",
    "\n",
    "* The solution to a differential equation is the function $y(t)$ that satisfies the relationship.\n",
    "* For an ordinary differential equation aka ODE all of the derivatives are with respect to a **single** independent variable.\n",
    "* For a partial differential equation aka PDE the derivatives are with respect to **multiple** independent variables.\n",
    "* The highest order derivative appearing in the ODE determines the **order**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Value Problems: Problem Statement\n",
    "The ODE $y' = f(t, y)$ does not determine a unique solution.\n",
    "\n",
    "The initial value $y(t_0) = y_0$ provided with the problem determines a unique solution to the ODE.\n",
    "* If the $y_i$ are specified at different values of $t$, then we have a **boundary value problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability\n",
    "\n",
    "---\n",
    "#### 1- Stable\n",
    "  * Solutions resulting from perturbations of initial value remain close to original solution.\n",
    "  * Example: $y'(t) = k$\n",
    "\n",
    "---\n",
    "#### 2- Asymptotically stable\n",
    "  * Solutions resulting from perturbations of initial value converge back to original solution.\n",
    "  * Example: $y'(t) = -y$\n",
    "\n",
    "---\n",
    "#### 3- Unstable\n",
    "  * Solutions resulting from perturbations of initial value diverge away from original solution without bound.\n",
    "  * Effects of the local error accumulate and as a result growth of global error is unbounded.\n",
    "  * Example: $y'(t) = y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Examples\n",
    "\n",
    "---\n",
    "#### Example-1: Scalar ODE\n",
    "Given ODE\n",
    "$$\n",
    "y' = \\lambda y\n",
    "$$\n",
    "\n",
    "Initial conditions: $y(t=0) = y_0$\n",
    "\n",
    "Solution\n",
    "$$\n",
    "y = y_0 e^{\\lambda t}\n",
    "$$\n",
    "\n",
    "For real $\\lambda$:\n",
    "* $\\lambda < 0$ asymptotically stable\n",
    "* $\\lambda > 0$ unstable\n",
    "\n",
    "For complex $\\lambda$:\n",
    "* Same as above with real component $\\text{Re}(\\lambda)$\n",
    "* $\\text{Re}(\\lambda) = 0$ stable, but not asymptotically stable\n",
    "\n",
    "---\n",
    "#### Example-2: Linear System ODE\n",
    "Given ODE\n",
    "$$\n",
    "y' = Ay\n",
    "$$\n",
    "where\n",
    "* $A$ is a $n \\times n$ diagonalizable matrix (eg $n$ linearly independent eigenvectors $v$)\n",
    "\n",
    "Initial conditions: $y(t=0) = y_0$ where $y_0 = \\sum_{i=1}^n \\alpha_i v_i$\n",
    "\n",
    "Solution\n",
    "$$\n",
    "y(t) = \\sum_{i=1}^n \\alpha_i v_i e^{\\lambda_i t}\n",
    "$$\n",
    "where\n",
    "* $\\alpha$ is taken from the linear combination of eigenvectors which forms $y_0$\n",
    "\n",
    "Eigenvalues $\\lambda$:\n",
    "* $\\lambda_i < 0$ asymptotically stable\n",
    "* $\\lambda_i > 0$ unstable\n",
    "* $\\lambda_i = 0$ oscillatory solution components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability and Accuracy\n",
    "\n",
    "Stability has nothing to do with accuracy.\n",
    "* An inaccurate method can be very stable.\n",
    "\n",
    "Stability determined by the following factors:\n",
    "* Differential equation being solved.\n",
    "* Method of solution.\n",
    "* Step size $h$.\n",
    "\n",
    "Accuracy of order $p$ means that the global error is a power of the step size $O(h^p)$.\n",
    "* Better accuracy obtained from higher order and smaller step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "Numerical methods for ODE incur 2 types of error:\n",
    "1. *Rounding error* due to finite precision floating-point\n",
    "2. *Truncation error* aka discretization error due to approximation method\n",
    "\n",
    "**Truncation error** is dominant factor in determining accuracy and composed of:\n",
    "1. *Global error* difference between computed solution and exact solution through initial point\n",
    "$$\n",
    "e_k = y_k - y(t_k)\n",
    "$$\n",
    "2. *Local error* difference between computed solution and solution passing through previous point\n",
    "$$\n",
    "l_k = y_k - u_{k-1}(t_k)\n",
    "$$\n",
    "\n",
    "If global error is **greater** than sum of local errors, then solution is **unstable**.\n",
    "\n",
    "If global error is **less** than sum of local errors, then solution is **stable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler's Method\n",
    "Euler's method is an **explicit** single-step method which advances solution by **extrapolating** along straight line of slope $f(t_k, y_k)$.\n",
    "$$\n",
    "y_{k+1} = y_k + h_k f(t_k, y_k)\n",
    "$$\n",
    "\n",
    "Given $t_0$ and $y_0$.\n",
    "1. Set $k=0$.\n",
    "2. Evaluate $f(t_k, y_k)$ to obtain the slope of trajectory.\n",
    "3. Update the time-step as $t_{k+1} = t_k + h_k$.\n",
    "4. Predict value at $y_{k+1}$ using $y_{k+1} = y_k + h_k f(t_k, y_k)$.\n",
    "5. Set $k=k+1$ and repeat from step 2.\n",
    "\n",
    "The step $y_k$ to $y_{k+1}$ adds some error to solution.\n",
    "* As a result, the solution at $y_{k+1}$ is on a different trajectory than the previous solution at $y_k$.\n",
    "* Stability of solutions determines whether the errors grow or diminish with increasing time step.\n",
    "\n",
    "Euler's method is stable if:\n",
    "* For scalar ODE, step size must satisfy $|1 + h\\lambda| \\leq 1$ or equivalently $h \\leq 2/\\lambda$.\n",
    "* For linear system ODE, $h \\leq 2/\\lambda_{\\text{max}}$ where $\\lambda_{\\text{max}}$ is the largest eigenvalue of the matrix of constant coefficients $A$.\n",
    "\n",
    "Accuracy: first-order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit Methods\n",
    "An **implicit** method evalutes $f(t_{k+1}, y_{k+1})$ before we know the value of $y_{k+1}$.\n",
    "* Use an iterative root-finding method to solve for $y_{k+1}$ using value of $y_k$ as initial guess.\n",
    "\n",
    "---\n",
    "#### 1- Backward Euler\n",
    "Given $t_0$ and $y_0$.\n",
    "1. Set $k=0$.\n",
    "2. Update the time-step as $t_{k+1} = t_k + h_k$.\n",
    "3. Determine the value of $y_{k+1}$ by solving $0 = y_k + h_k f(t_{k+1}, y_{k+1}) - y_{k+1}$ for $y_{k+1}$.\n",
    "4. Set $k=k+1$ and repeat from step 2.\n",
    "\n",
    "Backward Euler is unconditionally stable and first-order accurate.\n",
    "* Stable for any positive step size.\n",
    "* Only constraint on desired accuracy is the choice of step size.\n",
    "* Good for stiff ODE.\n",
    "\n",
    "Accuracy: first-order\n",
    "\n",
    "---\n",
    "#### 2- Trapezoid Method\n",
    "Higher accuracy can be obtained by averaging Euler and backward Euler methods.\n",
    "$$\n",
    "y_{k+1} = y_k + h_k \\frac{f(t_k, y_k) + f(t_{k+1}, y_{k+1})}{2}\n",
    "$$\n",
    "\n",
    "Trapezoid method is unconditionally stable and second-order accurate.\n",
    "* Better choice than Backward Euler.\n",
    "* Good for stiff ODE.\n",
    "\n",
    "Accuracy: second-order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stiffness\n",
    "An initial value problem is **stiff** if some components in the solution vector $y(t)$ vary much more rapidly with $t$ than others.\n",
    "* Stiffness results from a linear system ODE when there is a large disparity in the magnitudes of the positive eigenvalues.\n",
    "* Physical interpretation: components of a system have different time scales.  For example, a chemical reaction with one or more periods of very rapid transitions in concentration of constituent components.\n",
    "\n",
    "Stiffness is problem for numerical methods because the step size can be more severly restricted by stability than by accuracy.\n",
    "* Euler method is extremely inefficient for stiff ODE.\n",
    "* Backward Euler is suitable for stiff ODE because of unconditional stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runge-Kutta\n",
    "Single-step method that replaces higher derivatives of the Taylor series with finite difference approximations based on values of $f$ at points between $t_k$ and $t_{k+1}$.\n",
    "\n",
    "---\n",
    "#### 1- RK2 aka Heun's Method\n",
    "Analogous to trapezoid method:\n",
    "$$\n",
    "y_{k+1} = y_k + \\frac{h_k}{2}(k_1 + k_2)\n",
    "$$\n",
    "where\n",
    "* $k_1 = f(t_k, y_k)$\n",
    "* $k_2 = f(t_k + h_k, y_k + h_k k_1)$ (still explicit)\n",
    "\n",
    "Taylor Series: second-order\n",
    "\n",
    "---\n",
    "#### 2- RK4\n",
    "Analogous to Simpson's rule:\n",
    "$$\n",
    "y_{k+1} = y_k + \\frac{h_k}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n",
    "$$\n",
    "where\n",
    "* $k_1 = f(t_k, y_k)$\n",
    "* $k_2 = f(t_k + h_k/2, y_k + (h_k/2)k_1)$\n",
    "* $k_3 = f(t_k + h_k/2, y_k + (h_k/2)k_2)$\n",
    "* $k_4 = f(t_k + h_k, y_k + h_k k_3)$\n",
    "\n",
    "Taylor Series: fourth-order\n",
    "\n",
    "---\n",
    "Advantages\n",
    "* Self-starting eg no history of solution prior to time $t_k$ required to proceed to time $t_{k+1}$.\n",
    "* Easy to change step size $h_k$ during integration.\n",
    "\n",
    "Disadvantages\n",
    "* Provide no error estimate on which to base choice of step-size.\n",
    "  * Embedded Runge-Kutta method proposed by Dormand and Prince use differences of pairs of method of different order to estimate error and adjust step-size. \n",
    "* Inefficient for stiff ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 Topics Not Covered\n",
    "* Multistep Methods eg Predictor-Corrector\n",
    "* Multivalue Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Boundary Value Problems for ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Value Problems: Problem Statement\n",
    "The ODE $y' = f(t,y)$ does not determine a unique solution.\n",
    "* A kth order ODE requires $k$ side conditions $y_i$ where $i = 1, \\cdots, k$ to determine a unique solution.\n",
    "* If the $y_i$ are specified at values of $t \\neq 0$, then the conditions are referred to as **boundary conditions** (BC).\n",
    "\n",
    "General first-order two-point BVP with $f: \\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}^n$ given by: \n",
    "$$\n",
    "y' = f(t, y), \\qquad a < t < b\n",
    "$$\n",
    "\n",
    "and BC $g: \\mathbb{R}^{2n} \\rightarrow \\mathbb{R}^n$ given by:\n",
    "$$\n",
    "g(y(a), y(b)) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existence and Uniqueness\n",
    "\n",
    "Unlike an initial value problem, a BVP might not have a unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability\n",
    "\n",
    "For BVP the solution is determined everywhere simultaneously. \n",
    "* Potentially unstable in forward and backward directions with respect to time.\n",
    "* Compare to IVP, where stability only declines in forward direction with respect to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shooting Method\n",
    "Replace the BVP by a sequence of IVP formed by guessing values of $u'(a)$ until the second boundary condition $u(b)$ is satisfied.\n",
    "* Approximately satisfies ODE at each iteration.\n",
    "* Satisfies BC at convergence.\n",
    "\n",
    "Given BC $u(a) = \\alpha$ and $u(b) = \\beta$.\n",
    "1. **Guess** values for $u'(a)$ and solve the resulting IVP.\n",
    "2. Compare solution from previous step at $b$ with the BC $u(b) = \\beta$.\n",
    "3. Repeat step 1 until the solution obtained matches the BC at $b$.\n",
    "  * Use root-finding method to obtain the next guess for $u'(a)$ that is successively closer to $u(b) = \\beta$. \n",
    "\n",
    "Shooting method inherits stability of associated IVP.\n",
    "* Can be unstable even when IVP is stable.\n",
    "* Workaround: Multiple shooting. Divide integration interval into subintervals. Perform shooting on each subinterval.\n",
    "  * Disadvantage: Increases the amount of work to solve ODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite Difference Method\n",
    "Convert BVP into system of algebraic equations by replacing all derivatives with finite difference approximations on a mesh of points.\n",
    "* Approximately satisfies ODE upon convergence.\n",
    "* Satisfies BC at each iteration.\n",
    "\n",
    "Given $u'' = f(t, u, u')$ and $a < t < b$ with BC $u(a) = \\alpha$ and $u(b) = \\beta$.\n",
    "1. Compute mesh points $t_i = a + ih$ where step size $h = (b-a)/(n+1)$ and $i = 0, \\cdots, n+1$.\n",
    "2. Compute the finite difference approximation of derivative at each mesh point.\n",
    "$$\n",
    "u'(t_i) \\approx \\frac{y_{i+1} - y_{i-1}}{2h} \\\\\n",
    "u''(t_i) \\approx \\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2}\n",
    "$$\n",
    "3. Form the system of equations and solve for $y_i$ and $i = 1, \\cdots, n$.  The second two equations are used at the boundary conditions.\n",
    "$$\n",
    "\\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} = f \\left( t_i, y_i, \\frac{y_{i+1} - y_{i-1}}{2h} \\right) \\\\\n",
    "y_2 - 2y_1 + a = h^2 f \\left( x_1, y_1, \\frac{y_2 - y_a}{2h} \\right) \\\\\n",
    "b - 2y_n + y_{n+1} = h^2 f \\left( x_n, y_n, \\frac{b - y_{n-1}}{2h} \\right) \\\\\n",
    "$$\n",
    "\n",
    "The form of the system formed from the finite difference equations is **tridiagonal**.\n",
    "* A tridiagonal system of bandwidth $\\beta$ requires only $O(\\beta n)$ storage and $O(\\beta^2 n)$ work to factorize using LU factorization.\n",
    "\n",
    "The tridiagonal matrix will look something like:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & & & \\\\\n",
    "1 & -2f'' & 1 & & \\\\\n",
    "  & \\ddots & \\ddots & \\ddots & \\\\\n",
    "  & & 1 & -2f'' & 1 \\\\\n",
    "  & & & 2 & -2f'' \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_{n-1} \\\\\n",
    "y_n \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "u(a) \\\\\n",
    "f' \\\\\n",
    "\\vdots \\\\\n",
    "f' \\\\\n",
    "u(b) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Solution Values\n",
    "* Produces approximate values of the solution at mesh points.\n",
    "* Use interpolation to obtain the solution to the ODE at points other than mesh points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation Method\n",
    "Approximate solution to ODE by linear combinations of basis functions.\n",
    "* Requires solution to satisfy ODE at discrete set of collocation points.\n",
    "* Accuracy increases with number of collocation points.\n",
    "\n",
    "Given $u'' = f(t, u, u')$ and $a < t < b$ with BC $u(a) = \\alpha$ and $u(b) = \\beta$, seek a solution of the form:\n",
    "$$\n",
    "u(t) \\approx v(t, x) = \\sum_{i=1}^n x_i \\phi_i(t)\n",
    "$$\n",
    "where\n",
    "* $\\phi_i$ are basis functions (eg polynomials, trig functions, B-splines) defined on $[a, b]$\n",
    "  * Spectral method basis functions with global support (eg polynomials or trig functions)\n",
    "    * Dense (more work to solve) and potentially less well-conditioned.\n",
    "  * Finite element method basis functions with highly localized support (eg B-splines)\n",
    "    * Sparse (less work to solve) and generally well-conditioned.\n",
    "* $x$ is n-vector of parameters\n",
    "\n",
    "\n",
    "1. Define $n$ collocation points $a = t_1 < \\cdots < t_n = b$ at which $v(t, x)$ satisfies the ODE and BC.\n",
    "2. Form the $n \\times n$ system of equations and solve for $x_i$ and $i=2,\\cdots,n-1$. The second two equations are used at the boundary conditions.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v''(t_i, x) &= f(t_i, v(t_i, x), v'(t_i, x)) \\\\\n",
    "v(t_1, x) &= \\alpha \\\\\n",
    "v(t_n, x) &= \\beta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Solution Values\n",
    "* Force the approximate solution to satisfy the ODE exactly at the collocation points and BC.\n",
    "  * Satisfy means the solution has the same slope, but not necessarily same value.\n",
    "  * Residual is zero at the collocation points.\n",
    "* Use linear combination of $x$ and the basis functions to obtain the solution to the ODE at points other than collocation points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galerkin Method\n",
    "Rather than forcing residual to be zero at collocation points, minimize the residual over the entire interval of integration eg via least squares.\n",
    "\n",
    "Given $u'' = f(t, u, u')$ and $a < t < b$ with BC $u(a) = \\alpha$ and $u(b) = \\beta$, seek a solution that minimizes the residual:\n",
    "$$\n",
    "r(t, x) = \\sum_{i=1}^n x_i \\phi_i''(t) - f(t)\n",
    "$$\n",
    "where\n",
    "* $\\phi_i''$ is the second derivative basis functions for the approximate solution\n",
    "* $x$ is n-vector of parameters\n",
    "\n",
    "Using least squares we obtain a symmetric system of linear equations $Ax = b$.\n",
    "* The solution $x$ is the n-vector of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Topics Not Covered\n",
    "* Boundary Conditions: Separated vs. Linear\n",
    "* Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Partial Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Differential Equations: Notation\n",
    "* $u_t$ is PDE with one spatial variable $x$ and one time variable $t$\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}\n",
    "$$\n",
    "* $u_{xy}$ is PDE with two spatial variables $x$ and $y$\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial x \\partial y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Differential Equations: Problem Statement\n",
    "Second-order partial derivatives are classified according to:\n",
    "* form of discriminant (elliptic, parabolic, hyperbolic)\n",
    "* whether they are time-independent\n",
    "\n",
    "---\n",
    "#### 1- Elliptic\n",
    "Known as potential theory, describe harmonics and steady-state heat conduction. \n",
    "* No time component.\n",
    "\n",
    "Expressed in concise syntax as:\n",
    "$$\n",
    "u_{xx} + u_{yy} + \\lambda u = f(x,y)\n",
    "$$\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + \\lambda u = f(x, y)\n",
    "$$\n",
    "\n",
    "Special cases.\n",
    "##### 1.1- Poisson equation\n",
    "Expressed in concise syntax as:\n",
    "$$\n",
    "u_{xx} + u_{yy} + 0 = f(x,y)\n",
    "$$\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + 0 = f(x, y)\n",
    "$$\n",
    "\n",
    "##### 1.2- Laplace Equation\n",
    "Expressed in concise syntax as:\n",
    "$$\n",
    "u_{xx} + u_{yy} = 0\n",
    "$$\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "#### 2- Parabolic\n",
    "Describes how distribution of some quantity evolves with time in a solid medium. Diffusion. Time-dependent.\n",
    "* Example: Heat Equation\n",
    "\n",
    "Expressed in concise syntax as:\n",
    "$$\n",
    "u_t = c u_{xx}\n",
    "$$\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = c \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "#### 3- Hyperbolic\n",
    "Describes motion of a wave. Convection. Time-dependent.\n",
    "* Example: Wave Equation\n",
    "\n",
    "Expressed in concise syntax as:\n",
    "$$\n",
    "u_{tt} = c u_{xx}\n",
    "$$\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\left( \\frac{\\partial^2 u}{\\partial x_1^2} + \\frac{\\partial^2 u}{\\partial x_2^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Differential Equations: Convergence\n",
    "\n",
    "1. Consistency\n",
    "  * Local truncation error goes to zero.\n",
    "  * Avoids accumulation of local error into global error.\n",
    "2. Stability\n",
    "  * Approximate solution at any time $t$ as $\\Delta t \\rightarrow 0$ is bounded.\n",
    "3. Convergence\n",
    "  * Convergence occurs when the solution of a PDE approaches the true solution.\n",
    "\n",
    "Lax Equivalence Theorem\n",
    "* Consistency and stability necessary and sufficient for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Dependent Solution Methods\n",
    "Applications to heat (parabolic) and wave (hyperbolic) equations.\n",
    "\n",
    "---\n",
    "#### 1- Semidiscrete\n",
    "Discretize in space, but leave time continuous.\n",
    "\n",
    "#### 1.1- Method of Lines\n",
    "Approximate the solution to the PDE at $u(t_k, x_i)$ by solving the initial value problem at each of the $n$ spatial mesh points $x_i$ using finite differences.\n",
    "  * The system of ODEs is represented by a tridiagonal matrix and although computationally efficient, these are generally very stiff.\n",
    "\n",
    "#### 1.2- Finite Element\n",
    "Spatial discretization can also be done by finite element methods with local support.\n",
    "* Makes them “nearly” orthogonal, which tends to yield a relatively well-conditioned system of equations.\n",
    "* Makes the system sparse, so that much less work and storage are required to solve it.\n",
    "\n",
    "---\n",
    "#### 2- Fully Discrete\n",
    "Discretize in space and time.\n",
    "* Discrete mesh of points for all independent variables.\n",
    "* Replace all derivatives by finite difference approximations at points.\n",
    "* Numerical solution is a table of values at the mesh points.\n",
    "* Accuracy depends on the step size between mesh points.\n",
    "\n",
    "#### 2.1- Explicit Methods\n",
    "Explicit methods propagate the initial conditions forward in time.\n",
    "\n",
    "* Heat Equation\n",
    "  * Time: First order accurate\n",
    "  * Space: Second order accurate\n",
    "* Wave Equation\n",
    "  * Time: Second order accurate\n",
    "  * Space: Second order accurate\n",
    "\n",
    "Example: Heat Equation\n",
    "* Explicit fully discrete finite difference method.\n",
    "  * Forward difference in time (first-order)\n",
    "  * Centered difference in space (second-order).\n",
    "$$\n",
    "u_i^{k+1} = u_i^k + c \\frac{\\Delta t}{(\\Delta x)^2} \\left( u_{i+1}^k - 2u_i^k + u_{i-1}^k \\right)\n",
    "$$\n",
    "where\n",
    "* $k$ is time\n",
    "* $i$ is space\n",
    "\n",
    "#### 2.2- Implicit Methods\n",
    "Implicit methods such as backward Euler (first-order accurate) or Crank-Nicolson (second-order accurate) have larger stability region.\n",
    "* Larger stability region => larger step size.\n",
    "* Amount of work per step is larger to solve system of equations.\n",
    "\n",
    "Example: Heat Equation\n",
    "* Backward Euler, implicit fully discrete finite difference method.\n",
    "$$\n",
    "u_i^{k+1} = u_i^k + c \\frac{\\Delta t}{(\\Delta x)^2} \\left( u_{i+1}^{k+1} -2 u_i^{k+1} + u_{i-1}^{k+1} \\right)\n",
    "$$\n",
    "where\n",
    "* $k$ is time\n",
    "* $i$ is space\n",
    "\n",
    "Example: Heat Equation\n",
    "* Crank-Nicolson, implicit fully discrete finite difference method.\n",
    "  * Based on trapezoid method of ODE.\n",
    "$$\n",
    "u_i^{k+1} = u_i^k + c \\frac{\\Delta t}{2(\\Delta x)^2} \\left( u_{i+1}^{k+1} -2 u_i^{k+1} + u_{i-1}^{k+1} + u_{i+1}^k - 2 u_i^k + u_{i-1}^k \\right)\n",
    "$$\n",
    "where\n",
    "* $k$ is time\n",
    "* $i$ is space\n",
    "\n",
    "For implicit methods, the linear system to solve is tridiagonal with coefficients $[\\alpha, -2\\alpha-1, \\alpha]$ where $\\alpha$ is a constant multiple of the ratio of step sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Independent Solution Methods\n",
    "Obtain an approximate solution at all mesh points simultaneously by solving (large) sparse system of algebraic equations. \n",
    "\n",
    "Applications to elliptic PDE: Poisson and Laplace equations.\n",
    "\n",
    "**Note**: Let $u_{i,j}$ denote the approximate solution at $u(x_i, y_i)$.\n",
    "\n",
    "1. Define mesh points.\n",
    "  * Spatial mesh points $(x_i, y_j) = (ih, jh)$ and $i,j =1, \\cdots, n$ where $h = 1/(n+1)$.\n",
    "2. Replace $u_{xx}$ and $u_{yy}$ by centered difference of second derivative in space.\n",
    "$$\n",
    "\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0\n",
    "$$\n",
    "3. Using a `+`-shaped stencil of 4 points surrounding the central point $u_{1,1}$ we have the equation shown below.\n",
    "$$\n",
    "4 u_{1,1} - u_{0,1} - u_{2,1} - u_{1,0} - u_{1,2} = 0\n",
    "$$\n",
    "4. Repeat the stencil from the previous step to form the system of equations.\n",
    "  * Each equation involves 5 variables and resulting matrix is (sparse) block tridiagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Linear Systems: Direct vs. Iterative Methods\n",
    "1. Direct Methods\n",
    "  * Require no initial estimate.\n",
    "  * Produce high accuracy.\n",
    "  * Robust.\n",
    "2. Iterative Methods\n",
    "  * May require special properties eg CG => symmetric positive definite.\n",
    "  * May have poor rates of convergence eg Jacobi and Gauss-Seidel.\n",
    "  * More efficient if high accuracy not needed.\n",
    "  * Do not require explicit storage of matrix entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Methods for Linear Systems\n",
    "Iterative methods begin with initial guess for solution and continue until some termination criteria (example: $||b - Ax||$) is as small as desired.\n",
    "\n",
    "Choose $G$ and $c$ such that fixed point $Gx + c$ is a solution to $Ax = b$.\n",
    "$$\n",
    "x_{k+1} = G x_k + c\n",
    "$$\n",
    "\n",
    "Method called **stationary** since $G$ and $c$ are fixed over all iterations.\n",
    "\n",
    "Converges if spectral radius of the Jacobian matrix $G$ is less than 1 eg $\\rho(G) < 1$.\n",
    "* Rate of convergence increases as spectral radius $\\rho(G)$ decreases.\n",
    "\n",
    "---\n",
    "#### 1- Splitting\n",
    "Rewrite $A$ as $A = M - N$.\n",
    "\n",
    "Iteration scheme: \n",
    "$$\n",
    "x_{k+1} = M^{-1} N x_k + M^{-1} b\n",
    "$$\n",
    "\n",
    "---\n",
    "#### 2- Jacobi\n",
    "Splitting of $A$ is given by:\n",
    "  * $M = D$ where $D$ is formed from the diagonals of $A$.\n",
    "  * $N = -(L + U)$ where $L$ and $U$ contain the lower and upper triangular of $A$.\n",
    "\n",
    "Iteration scheme: \n",
    "$$\n",
    "x_{k+1} = D^{-1}(b - (L + U)x_k)\n",
    "$$\n",
    "\n",
    "Jacobi method will fail if $D$ is singular.\n",
    "\n",
    "Jacobi methods requires double storage for $x$ to hold all values from previous iteration.\n",
    "* Makes algorithm easier to parallelize.\n",
    "\n",
    "---\n",
    "#### 3- Gauss-Seidel\n",
    "Splitting of $A$ is given by:\n",
    "  * $M = D + L$\n",
    "  * $N = -U$\n",
    "\n",
    "Iteration scheme: \n",
    "$$\n",
    "x_{k+1} = (D + L)^{-1}(b - Ux_k)\n",
    "$$\n",
    "\n",
    "Gauss-Seidel does not require any extra storage for $x$.\n",
    "\n",
    "$(D + L)$ is triangular system and will require additonal work to invert compared to $D^{-1}$.\n",
    "\n",
    "---\n",
    "#### 4- Successive Over-Relaxation (SOR)\n",
    "Convergence rate of Gauss-Seidel accelerated by using weighted average of current iterate and next Gauss-Seidel iterate.\n",
    "\n",
    "Iteration scheme:\n",
    "$$\n",
    "x_{k+1} = (1 - \\omega)x_k + \\omega x_{k+1,GS}\n",
    "$$\n",
    "where\n",
    "* $\\omega$ is the weight given to current and next iterate, $0 < \\omega < 2$.\n",
    "  * $\\omega > 1$ gives over-relaxation.\n",
    "  * $\\omega < 1$ gives under-relaxation.\n",
    "  * $\\omega = 1$ is identical to Gauss-Seidel.\n",
    "\n",
    "---\n",
    "#### 5- Conjugate Gradient\n",
    "If $A$ is symmetric positive definite, then $\\phi(x) = \\frac{1}{2} x^T A x - x^T b$ attains a minimum when $Ax = b$.\n",
    "\n",
    "Optimization methods have form:\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha s_k\n",
    "$$\n",
    "where\n",
    "* $\\alpha$ is search parameter chosen to minimize objective function along $s_k$.\n",
    "\n",
    "For optimization, use negative gradient as the residual vector $r$.\n",
    "$$\n",
    "r = - \\nabla \\phi(x) = b - Ax\n",
    "$$\n",
    "\n",
    "Search parameter $\\alpha$ follows from $r$ as $\\alpha = r_k^T s_k / s_k^T A s_k$.\n",
    "\n",
    "This approach only requires a routine for computing $Ax$, hence attractive for solving large sparse linear systems.\n",
    "\n",
    "When CG is used as a direct method, the rounding error causes a loss of orthogonality.  \n",
    "* As a result, CG is usually used in an iterative manner and halted when the residual is sufficiently small.\n",
    "\n",
    "CG has been generalized to nonsymmetric systems, but less robust and requires more storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence of Iterative Methods\n",
    "Convergence rates: Jacobi < Gauss-Seidel < SOR < CG\n",
    "* Jacobi and Gauss-Seidel are impractical for large problems.\n",
    "* Convergence of SOR depends on hyperparameter $\\omega$ which can be hard to determine.\n",
    "* Convergence rate of CG method can be furthered improved with **preconditioners**.\n",
    "\n",
    "**Smoothers**: Stationary iterative methods exhibit asymptotic convergence.\n",
    "* Make rapid initial progress to remove high-frequency error.\n",
    "* Make slow progress to remove low-frequency error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Topics Not Covered\n",
    "* Domains of Dependence (DoD)\n",
    "* Multigrid Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Series: Background\n",
    "Represent a continous function as linear combination of sines and cosines.\n",
    "* The sine and cosine components are referred to as **frequencies**.\n",
    "* For some problems, frequency domain is more efficient than time or space domain.\n",
    "  * Compressed representation for the repeating parts of the sequence.\n",
    "  * Overlapping cyclic phenomena are more easily separated in the frequency domain.\n",
    "\n",
    "**Euler's formula**\n",
    "\n",
    "aka **complex exponential**\n",
    "$$\n",
    "e^{i \\theta} = \\cos \\theta + i \\sin \\theta\n",
    "$$\n",
    "where\n",
    "* $i = \\sqrt{-1}$ is complex\n",
    "\n",
    "**Roots of Unity**\n",
    "\n",
    "(def.) Root $x$ that satisfies the equation $x^n = 1$.\n",
    "\n",
    "aka **twiddle factors**\n",
    "\n",
    "For a given integer $n$ the primitive nth root of unity is given by:\n",
    "$$\n",
    "\\omega_n = \\cos(2\\pi/n) - i \\sin(2\\pi/n) = e^{-2\\pi i/n}\n",
    "$$\n",
    "\n",
    "##### Example: $w_4^k$ \n",
    "$k = [0, 1, 2, 3]$ represents $\\pi/2$ steps counterclockwise around the real-complex plane.\n",
    "* $w_4^0 = 1$\n",
    "* $w_4^1 = -i$\n",
    "* $w_4^2 = -1$\n",
    "* $w_4^3 = i$\n",
    "\n",
    "Note: Exponent starts from $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Fourier Transform\n",
    "DFT gives a trigonometric interpolant using only matrix-vector multiplication with $O(n^2)$ work.\n",
    "* DFT of purely real sequence is in general complex.\n",
    "\n",
    "---\n",
    "#### 1- Forward DFT\n",
    "DFT $y$ of the sequence $x = [x_0, \\cdots, x_{n-1}]^T$ is given by:\n",
    "$$\n",
    "y_m = \\sum_{k=0}^{n-1} {x_k \\omega_n^{mk}}, \\qquad m = 0, \\cdots, n-1\n",
    "$$\n",
    "where\n",
    "* $\\omega_n^{mk}$ is the kth element of the nth-root of unity\n",
    "\n",
    "Expressed in matrix notation:\n",
    "$$\n",
    "y = F_n x\n",
    "$$\n",
    "where\n",
    "* $F_n$ is the Fourier matrix with entries $F_n = \\omega_n^{mk}$.\n",
    "  * $F_n$ is symmetric Vandermonde matrix\n",
    "\n",
    "---\n",
    "#### 2- Inverse DFT\n",
    "Inverse DFT $x$ of the sequence $y = [y_0, \\cdots, y_{n-1}]^T$ is given by:\n",
    "$$\n",
    "x_k = \\frac{1}{n} \\sum_{m=0}^{n-1} {y_m \\omega_n^{-mk}}, \\qquad k = 0, \\cdots, n-1\n",
    "$$\n",
    "\n",
    "Expressed in matrix notation:\n",
    "$$\n",
    "x = F_n^{-1} y\n",
    "$$\n",
    "where\n",
    "* $F_n^{-1} = (1/n) F_n^H$ is the inverse of the Fourier matrix\n",
    "  * $F_n^H$ is the conjugate transpose of $F_n$, so $(1/\\sqrt{n}) F_n$ is unitary\n",
    "    * Note: $F_n$ is almost but **not** Hermitian\n",
    "\n",
    "---\n",
    "$y_0 = \\sum_{k=0}^{n-1} x_k$ is called zero frequency or **DC** component\n",
    "\n",
    "$y_{n/2}$ is highest frequency representable aka **Nyquist frequency**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Fourier Transform\n",
    "FFT uses recursive divide-and-conquer algorithm to compute the DFT more efficiently.\n",
    "* Work required $O(n \\log_2 n)$.\n",
    "  * Compare to $O(n^2)$ for matrix-vector product form of DFT.\n",
    "* Can be implemented in-place and using no additional storage.\n",
    "* FFT can be reworked to compute inverse DFT.\n",
    "\n",
    "Assumptions of Input Sequence\n",
    "1. Equally spaced.\n",
    "2. Periodic.\n",
    "  * Transforming non-periodic sequence may introduce spurious noise.\n",
    "3. Power of 2 in length.\n",
    "  * Padding a sequence may introduce spurious noise.\n",
    "  * **Mixed-radix FFT** is a work-around for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compact implementation of FFT shown below uses twice the storage and is used to compute the FFT of the sequence $x = [4,0,3,6,2,9,6,5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def roots_unity(n):\n",
    "    \"\"\"\n",
    "    Return the nth roots of unity $e^{(-2k \\pi i)/n}$.\n",
    "    \"\"\"\n",
    "    k = np.arange(0, n, dtype='d')\n",
    "    return np.exp((-2.*k*np.pi*1j)/n)\n",
    "\n",
    "\n",
    "def fft(x, n, omega):\n",
    "    \"\"\"\n",
    "    Compute the DFT of the sequence x using the FFT.\n",
    "    \n",
    "    x is the input sequence.\n",
    "    n is the number of points in the sequence, must be 2^n.\n",
    "    omega are the nth roots of unity.\n",
    "    \n",
    "    Returns y as the result of applying the DFT to x.\n",
    "    \"\"\"\n",
    "    if n == 1:  # Base case.\n",
    "        return np.array([x[0]])\n",
    "    halfn = n//2\n",
    "    # Split x into odd and even sequences.\n",
    "    odd, even = np.zeros(halfn), np.zeros(halfn)\n",
    "    for k in range(halfn):\n",
    "        odd[k], even[k] = x[2*k+1], x[2*k]\n",
    "    # Recursively compute DFT of each sequence.\n",
    "    yodd = fft(odd, halfn, omega*omega)\n",
    "    yeven = fft(even, halfn, omega*omega)\n",
    "    # Combine results.\n",
    "    y = np.zeros(n, dtype=np.complex)\n",
    "    for k in range(n):\n",
    "        y[k] = omega[k]*yodd[k%halfn] + yeven[k%halfn]\n",
    "    return y\n",
    "\n",
    "\n",
    "# Initialize the input sequence and compute FFT.\n",
    "x = np.array([4,0,3,6,2,9,6,5], dtype=np.float64)\n",
    "n = x.size\n",
    "omega = roots_unity(n)\n",
    "y = fft(x, n, omega)\n",
    "\n",
    "# Compare the result of FFT with numpy.fft.fft.\n",
    "expected = np.fft.fft(x, n)\n",
    "np.testing.assert_almost_equal(y, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of DFT\n",
    "\n",
    "---\n",
    "#### 1- Remove High Frequency Noise\n",
    "1. Compute DFT of the sequence.\n",
    "2. Set high-frequency components of the transformed sequence to zero.\n",
    "3. Compute the inverse DFT of the sequence to transform data back to time domain.\n",
    "\n",
    "---\n",
    "#### 2- Discrete Convolution\n",
    "1. Transform the input sequence and kernel to frequency domain using DFT.\n",
    "  * Zero-pad the kernel to be of the same size as the input sequence.\n",
    "2. Compute the pointwise product of the two sequences in frequency domain.\n",
    "3. Transform the product back to the time domain using inverse FFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 Topics Not Covered\n",
    "* Wavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 Stochastic Simulation and Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "Mimic the behavior of a system by exploiting randomness to obtain a statistical sample of possible outcomes.\n",
    "\n",
    "Use Cases\n",
    "1. Nondeterministic systems\n",
    "2. Systems too complicated to model analytically\n",
    "3. Deterministic systems with high dimensionality.\n",
    "  * Example: Monte Carlo integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random vs. Quasi-Random\n",
    "\n",
    "---\n",
    "#### 1-Random\n",
    "A sequence is random if it has no shorter description than itself.\n",
    "\n",
    "---\n",
    "#### 2- Quasi-Random\n",
    "A sequence which provides uniform coverage while maintaining reasonablly random appearance.\n",
    "  * Increasingly favored over uniform random number generators for Monte Carlo methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Number Generators: Uniform Distribution\n",
    "Properties of a Good Random Number Generator\n",
    "1. Pass statistical tests of randomness.\n",
    "2. Long period.\n",
    "    * Period (def.) Length of sequence before starts repeating.\n",
    "3. Efficient.\n",
    "    * Execute quickly and hold little state.\n",
    "4. Repeatability.\n",
    "    * Produces repeatable sequence starting from same seed.\n",
    "5. Portability.\n",
    "    * Produce the same sequence from same seed on different computers.\n",
    "\n",
    "---\n",
    "#### 1- Congruential Generators\n",
    "A congruential generator produces a sequence of integers in interval $[0, M-1]$:\n",
    "$$\n",
    "x_k = (a x_{k-1} + b) (\\bmod M)\n",
    "$$\n",
    "where\n",
    "* $a, b$ are integer constants associated with the generator.\n",
    "* $x_{k-1}$ is the previous value in the sequence or a seed if new sequence.\n",
    "* $M$ is an integer constant equal to the largest representable integer.\n",
    "\n",
    "Period of congruential generator cannot exceed $M$.\n",
    "\n",
    "---\n",
    "#### 2- Fibonacci Generators\n",
    "Produce floating point random numbers on interval $[0, 1]$ as difference or sum of previous values.\n",
    "$$\n",
    "x_k = x_{k-M} - x_{k-P}\n",
    "$$\n",
    "where\n",
    "* $M$ and $P$ are referred to as **lag**\n",
    "\n",
    "Fibonacci generators require special initialization in order to compute first terms without lag.\n",
    "\n",
    "Fibonacci generators have longer period than congruential generators since repitition of one sequence does not entail all subsequent members will repeat in same order.\n",
    "\n",
    "Fibonacci generators do not require floating point division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Number Generators: Nonuniform Distributions\n",
    "A nonuniform generator can be built from a uniform generator:\n",
    "1. Compute the inverse of the PDF of the distribution.\n",
    "2. Pass the uniform random deviate to the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Number Generators: Translate and Scale\n",
    "\n",
    "---\n",
    "#### 1- Uniform Distribution\n",
    "Transform the uniform random number $x_k$ on the interval $[0, 1)$ to a uniform random number on the interval $[a, b)$.\n",
    "$$\n",
    "x_k' = a + (b-a)x_k\n",
    "$$\n",
    "\n",
    "---\n",
    "#### 2- Normal Distribution\n",
    "Transform the normally distributed random number $x_k$ with $N(\\mu=0, \\sigma^2=1)$ to a normally distributed number with $N(\\mu', \\sigma'^2)$.\n",
    "$$\n",
    "x_k' = \\mu' + \\sigma' x_k\n",
    "$$\n",
    "where\n",
    "* $\\sigma'$ is the standard deviation, $\\sqrt{\\sigma'^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 Topics Not Covered\n",
    "None; all topics covered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
